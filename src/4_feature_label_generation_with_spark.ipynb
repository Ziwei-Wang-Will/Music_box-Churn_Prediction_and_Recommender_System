{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load data into Spark DataFrame and perform brief data exploration\n",
    "2. Label definition\n",
    "3. Data cleaning  \n",
    "  3.1 Data processing of \"song_length\"  \n",
    "  3.2 Data processing of \"play_time\"  \n",
    "4. Feature generation  \n",
    "  4.1 Generate Features A: Play time percentage related features  \n",
    "  4.1.1 Generate Features A1: Proportion features of different level of play time percentage  \n",
    "  4.1.2 Generate Features A2: Acceleration features of different level of play time percentage    \n",
    "  \n",
    "  4.2 Generate Feature B: Play_time related feature  \n",
    "  4.2.1 Generate Features B1: Total play_time  \n",
    "  4.2.2 Generate Features B2: Acceleration features of total play_time  \n",
    "  4.2.3 Generate Features B3: Average play_time of songs  \n",
    "  \n",
    "  4.3 Generate Features C: Event related features  \n",
    "  4.3.1 Generate Event features C1: events frequency in given windows  \n",
    "  4.3.2 Generate Event features C2:  Ratio of event frequency of nearest 7 days to that of nearest 30 days  \n",
    "  \n",
    "  4.4 Generate Features D: Recency related features  \n",
    "  4.4.1 Generate Recency features D1: Last Event Time from feature_window_end_date  \n",
    "  \n",
    "  4.5 Generate Features E: Profile related features  \n",
    "  4.5.1 Generate Profile features E1: device_feature  \n",
    "5. Form training data for prediction models  \n",
    "6. Form rating data for recommendation system  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data into Spark DataFrame and perform brief data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data into Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[uid: string, event: string, device: string, song_id: string, date: string, play_time: string, song_length: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.csv('../data/event_ds.csv',header=True).cache()\n",
    "# cache(): run only when there is a action\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[uid: string, event: string, device: string, song_id: string, date: date, play_time: string, song_length: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new or overwrite original field with withColumn\n",
    "df = df.withColumn('date',F.col('date').cast('date'))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------+-------+----------+---------+-----------+\n",
      "|      uid|event|device|song_id|      date|play_time|song_length|\n",
      "+---------+-----+------+-------+----------+---------+-----------+\n",
      "|168540455|    P|    ar| 298250|2017-03-30|      189|        190|\n",
      "|168535490|    P|    ar|6616004|2017-03-30|      283|        283|\n",
      "|168530895|    P|    ar|      0|2017-03-30|      264|        265|\n",
      "|168551548|    P|    ar|1474915|2017-03-30|        5|        243|\n",
      "|168551509|    P|    ar|6329735|2017-03-30|      289|        289|\n",
      "+---------+-----+------+-------+----------+---------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Briefly data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of entries: 12445200\n"
     ]
    }
   ],
   "source": [
    "# simple count rows\n",
    "print(\"total number of entries:\", df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of users: 59754\n"
     ]
    }
   ],
   "source": [
    "# select operation, count distinct rows: number of user_id \n",
    "print(\"total number of users:\", df.select('uid').distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|event|   count|\n",
      "+-----+--------+\n",
      "|    D|  646004|\n",
      "|    S|  779581|\n",
      "|    P|11019615|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# group by aggregation\n",
    "df.groupBy('event').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "| song_id| count|\n",
      "+--------+------+\n",
      "|       0|914738|\n",
      "|    null|785038|\n",
      "| 9950164| 87436|\n",
      "|15249349| 55854|\n",
      "| 5237384| 41825|\n",
      "+--------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "The most popular songs as above None\n"
     ]
    }
   ],
   "source": [
    "print(\"The most popular songs as above\", \n",
    "df.groupby('song_id').count().sort(F.col('count').desc()).show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|      uid|count|\n",
      "+---------+-----+\n",
      "|168954949| 7131|\n",
      "|167925318| 6829|\n",
      "|167979374| 6343|\n",
      "|168416042| 5564|\n",
      "|168442087| 5447|\n",
      "+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "The most active user_id as above None\n"
     ]
    }
   ],
   "source": [
    "print(\"The most active user_id as above\", \n",
    "df.groupby('uid').count().sort(F.col('count').desc()).show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Label definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We define churn and time windows as below: \n",
    "#### 1 for  churn: user has P/D/S entries in 'Feature window' while has no P/D/S entries in 'Label window'\n",
    "#### 0 for not churn: user has P/D/S entries in both two windows\n",
    "#### Label window: 2017-04-29 ~ 2017-05-12 days: 14\n",
    "#### Feature window: 2017-03-30 ~ 2017-04-28 days: 30\n",
    "##### note: here we ignore user who has P/D/S entries in ' Label window' while has no P/D/S entries in 'Feature window'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label window: 2017-04-29 ~ 2017-05-12 days: 14\n",
      "feature window: 2017-03-30 ~ 2017-04-28 days: 30\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from dateutil import parser\n",
    "\n",
    "label_window_size = 14\n",
    "label_window_end_date = parser.parse('2017-05-12').date()\n",
    "label_window_start_date = label_window_end_date - datetime.timedelta(label_window_size - 1)\n",
    "print('label window:',label_window_start_date,'~',label_window_end_date,'days:',label_window_size)\n",
    "\n",
    "feature_window_size = 30\n",
    "feature_window_end_date = label_window_start_date - datetime.timedelta(1)\n",
    "feature_window_start_date = feature_window_end_date  - datetime.timedelta(feature_window_size - 1)\n",
    "print('feature window:',feature_window_start_date,'~',feature_window_end_date,'days:',feature_window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the uid in feature window we will model\n",
    "df_model_uid = df.filter((F.col('date')>=feature_window_start_date) & (F.col('date')<=feature_window_end_date))\\\n",
    "                    .select('uid').distinct()\n",
    "# active uid in label window (active label=0)\n",
    "df_active_uid_in_label_window = df.filter((F.col('date')>=label_window_start_date) & (F.col('date')<=label_window_end_date))\\\n",
    "                            .select('uid').distinct().withColumn('label',F.lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size of users in feature window is 58432\n"
     ]
    }
   ],
   "source": [
    "# prepare label data (churn label=1; active label=0)\n",
    "df_label = df_model_uid.join(df_active_uid_in_label_window,on=['uid'],how='left')\n",
    "df_label = df_label.fillna(1)   # 1 for churn\n",
    "print(\"Sample size of users in feature window is\", df_label.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1|36272|\n",
      "|    0|22160|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_label.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We have 36272 churn label and 22160 active label, labels are comparable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# event_data in feature_window\n",
    "df_feature_window = df.filter((F.col('date')>=feature_window_start_date) & (F.col('date')<=feature_window_end_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9557651"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feature_window.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------+-------+--------------------+-------------------+-------------------+\n",
      "|summary|                 uid|  event| device|             song_id|          play_time|        song_length|\n",
      "+-------+--------------------+-------+-------+--------------------+-------------------+-------------------+\n",
      "|  count|             9557651|9557651|9557651|             8934375|            8391865|            8393403|\n",
      "|   mean|1.6642767226301807E8|   null|   null| 1.55567109663087E14|  28226.20623478178|-1231.9704732739265|\n",
      "| stddev|1.4495014041825246E7|   null|   null|4.486923339065008...|7.927618484536014E7| 1820892.4337731672|\n",
      "|    min|           100077577|      D|     ar|                  -1|       -0.011976957|                 -1|\n",
      "|    max|            99850419|      S|     ip|             9999722|                nan|                999|\n",
      "+-------+--------------------+-------+-------+--------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_feature_window.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------+-------+--------------------+-------------------+-------------------+\n",
      "|summary|                 uid|  event| device|             song_id|          play_time|        song_length|\n",
      "+-------+--------------------+-------+-------+--------------------+-------------------+-------------------+\n",
      "|  count|             8395152|8395152|8395152|             8391657|            8391865|            8393403|\n",
      "|   mean|1.6628846441030693E8|   null|   null|1.656281819333759...|  28226.20623478178|-1231.9704732739265|\n",
      "| stddev| 1.500893981882352E7|   null|   null|4.629741133728094...|7.927618484536014E7| 1820892.4337731672|\n",
      "|    min|           100077577|      P|     ar|                  -1|       -0.011976957|                 -1|\n",
      "|    max|            99850419|      P|     ip|             9999722|                nan|                999|\n",
      "+-------+--------------------+-------+-------+--------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_play = df_feature_window.filter(F.col('event') == 'P') \n",
    "df_play.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We notice some issue with our data as below: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1/ We need to check whether there are songs with zero or negative 'song_length', if so, we need to manipulate these values.  \n",
    "2/ We notice that the mean value of 'song_length' is negative, there may be some outliers, we need to exclude them.  \n",
    "3/ We notice that  in column 'play_time', there are 'nan' and negative value, we need to manipulate these values.  \n",
    "4/ We notice that the mean value of 'play_time' is incredibly large, there may be some outliers, we need to exclude them.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data processing of  \"song_length\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now we will try to solve the following problem: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1/ We need to check whether there are songs with zero or negative 'song_length', if so, we need to manipulate these values.   \n",
    "2/ We notice that the mean value of song_length' is incredibly small, there may be some outliers, we need to exclude them.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### count 'nan' song_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1749 songs with NULL song_lenght\n"
     ]
    }
   ],
   "source": [
    "# count null song_lenght\n",
    "print(\"There are\", \n",
    "      df_play.filter(F.col('song_length').isNull()).count(),\\\n",
    "     \"songs with NULL song_lenght\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's check whether these NULL song_length also have NULL song_id and NULL play_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1749 songs with NULL song_lenght, song_id and play_time\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\", \n",
    "      df_play.filter(F.col('song_length').isNull()).filter(F.col('song_id').isNull()).filter(F.col('play_time').isNull()).count(),\\\n",
    "     \"songs with NULL song_lenght, song_id and play_time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We find that songs with NULL song_lenght also have NULL song_id and NULL play_time. Let's count these odd songs by uid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These odd songs are played by 17 uid\n"
     ]
    }
   ],
   "source": [
    "odd_songs_by_uid = df_play.filter(F.col('song_length').isNull()) \\\n",
    "                                                      .filter(F.col('song_id').isNull()) \\\n",
    "                                                      .filter(F.col('play_time').isNull()) \\\n",
    "                                                      .groupBy(F.col('uid')).count().sort(F.col('count').desc())\n",
    "print(\"These odd songs are played by\", \n",
    "      odd_songs_by_uid.count(),\\\n",
    "     \"uid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As these odd songs are played by only 17 uid, we can remove these records directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_play_song_length_without_nan = df_play.filter(F.col('song_length').isNotNull())\n",
    "df_feature_window_cleaned = df_feature_window.filter(F.col('event') == 'P').filter(F.col('song_length').isNotNull()) \\\n",
    "                                  .union(df_feature_window.filter(F.col('event') == 'D')) \\\n",
    "                                  .union(df_feature_window.filter(F.col('event') == 'S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### to check if there are songs with “song_length == 0” while “play_time ≠ 0”, if so, we need to deal with  “song_length == 0”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 287252 songs with “song_length == 0” while “play_time ≠ 0”\n"
     ]
    }
   ],
   "source": [
    "# to check if there are songs with “song_length == 0” while “play_time ≠ 0”\n",
    "df_play_song_length_without_nan = df_play_song_length_without_nan \\\n",
    "                                            .withColumn('song_length', F.col('song_length').cast('int'))\n",
    "\n",
    "print(\"There are\", \n",
    "      df_play_song_length_without_nan.filter((F.col('song_length') == 0) & (F.col('play_time') != 0)).count(),\\\n",
    "     \"songs with “song_length == 0” while “play_time ≠ 0”\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We need to deal with the the songs with “song_length == 0”, while before that, I need to check whether the churn rate of songs with “song_length == 0” and that of songs with “song_length !=0” are significant different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to calculate label_column rate of different feature_column value\n",
    "def whether_churn_rate_significant_diff_with_zero(df, feature_column, label_column):\n",
    "    feature_column_vs_label = df \\\n",
    "                 .withColumn(\"feature_column_unnormal\", F.when(F.col(feature_column) == 0, 1).otherwise(0)) \\\n",
    "                 .agg(F.sum(F.when((F.col(\"feature_column_unnormal\") == 1) & (F.col(label_column) == 1),1).otherwise(0)).alias('feature_column_unnormal_label_column'),\n",
    "                      F.sum(F.when((F.col(\"feature_column_unnormal\") == 1) & (F.col(label_column) == 0),1).otherwise(0)).alias('feature_column_unnormal_not_label_column'),\n",
    "                      F.sum(F.when((F.col(\"feature_column_unnormal\") == 0) & (F.col(label_column) == 1),1).otherwise(0)).alias('feature_column_normal_label_column'),\n",
    "                      F.sum(F.when((F.col(\"feature_column_unnormal\") == 0) & (F.col(label_column) == 0),1).otherwise(0)).alias('feature_column_normal_not_label_column'))\n",
    "    feature_column_vs_label = feature_column_vs_label.select(F.round(F.col('feature_column_unnormal_label_column') / (F.col('feature_column_unnormal_label_column') + F.col('feature_column_unnormal_not_label_column')),2).alias(feature_column + '_unnormal_churn_rate'),\\\n",
    "                                                     F.round(F.col('feature_column_normal_label_column') / (F.col('feature_column_normal_label_column') + F.col('feature_column_normal_not_label_column')),2).alias(feature_column + '_normal_churn_rate'))\n",
    "    return feature_column_vs_label   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-----------------------------+\n",
      "|song_length_unnormal_churn_rate|song_length_normal_churn_rate|\n",
      "+-------------------------------+-----------------------------+\n",
      "|                           0.32|                         0.26|\n",
      "+-------------------------------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show whether the churn rate of songs with '0' song_length and that of songs with “song_length >=0” are significant different\n",
    "df_play_song_length_without_nan = df_play_song_length_without_nan.join(df_label,on='uid',how='left')\n",
    "whether_churn_rate_significant_diff_with_zero(df_play_song_length_without_nan, \"song_length\", \"label\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From the table above we find the churn rate of songs with “song_length == 0” and that of songs with “song_length !=0” are not significant different, which means “song_length == 0” appears randomly without pattern, so that we can replace them with mean value of column \"song_length\" after we remove the outlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Get the 99th of song_length as 'top_song_length_threshold', and calculate mean value of larger or equal to zero \"song_length\" records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order column 'play_time' and sort, then find 99% top value\n",
    "df_play_song_length_larger_or_equals_to_zero = df_play_song_length_without_nan.filter(F.col('song_length') >= 0)\n",
    "\n",
    "top_song_length_threshold_cut = int(df_play_song_length_larger_or_equals_to_zero.count() * 0.01)    # top 1%\n",
    "\n",
    "df_play_song_length_larger_or_equals_to_zero_cut = df_play_song_length_larger_or_equals_to_zero \\\n",
    "                                                        .sort(F.col(\"song_length\").desc()) \\\n",
    "                                                        .select(F.col('song_length')) \\\n",
    "                                                        .limit(top_song_length_threshold_cut)     # limit top 1% play_time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99th of song_length as top_song_length_threshold to remove outliers is 1333\n"
     ]
    }
   ],
   "source": [
    "# get the 99th of song_length as top_song_length_threshold to remove outliers.\n",
    "top_song_length_threshold = df_play_song_length_larger_or_equals_to_zero_cut.sort(F.col(\"song_length\")).take(1)[0][0]\n",
    "print('99th of song_length as top_song_length_threshold to remove outliers is', str(top_song_length_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean value of larger or equal to zero \"song_length\" records is 223.0\n"
     ]
    }
   ],
   "source": [
    "# filter song_length less than 'top_song_length_threshold' \n",
    "# then calculate mean value of song_length\n",
    "\n",
    "from pyspark.sql.functions import col, avg\n",
    "\n",
    "song_length_mean = df_play_song_length_larger_or_equals_to_zero\\\n",
    "                            .withColumn(\"song_length_is_outlier\", F.when(F.col('song_length') >= top_song_length_threshold, 1).otherwise(0)) \\\n",
    "                            .filter(F.col(\"song_length_is_outlier\") == 0) \\\n",
    "                            .agg(F.round(avg(col(\"song_length\")), 0).alias('song_length_mean'))\n",
    "\n",
    "song_length_mean = song_length_mean.take(1)[0][0]\n",
    "print('mean value of larger or equal to zero \"song_length\" records is', str(song_length_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Replace negative and zero song_length with  'song_length_mean',  and remove outlier whose 'song_length' value less than 'top_song_length_threshold'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace negative and zero song_length with 'song_length_mean', and remove song_length outlier\n",
    "\n",
    "df_play_song_length_cleaned = df_play_song_length_without_nan \\\n",
    "                            .withColumn('song_length', F.when(F.col('song_length') <= 0, song_length_mean).otherwise(F.col('song_length'))) \\\n",
    "                            .filter(F.col(\"song_length\") <= top_song_length_threshold) \n",
    "\n",
    "df_feature_window_cleaned = df_feature_window_cleaned.withColumn('song_length', F.col('song_length').cast('int'))\\\n",
    "                                  .filter(F.col('event') == 'P').filter(F.col('song_length') <= top_song_length_threshold) \\\n",
    "                                  .union(df_feature_window.filter(F.col('event') == 'D')) \\\n",
    "                                  .union(df_feature_window.filter(F.col('event') == 'S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We finished the cleaning of 'song_length'. \n",
    "### 3.2 Data processing of  \"play_time\"\n",
    "##### Next, we will solve the following two problems:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3/ We notice that  in column 'play_time', there are 'nan' and negative value, we need to manipulate these values.  \n",
    "4/ We notice that the mean value of 'play_time' is incredibly large, there may be some outliers, we need to exclude them.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### count NULL play_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1183 songs with NULL play_time\n"
     ]
    }
   ],
   "source": [
    "# count NULL play_time\n",
    "print(\"There are\", \n",
    "      df_play_song_length_cleaned.filter(F.col('play_time').isNull()).count(),\\\n",
    "     \"songs with NULL play_time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Before I deal with the songs with NULL play_time, I need to check whether the churn rate of songs with NULL play_time and that of songs with “play_time >=0” are significant different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whether_churn_rate_significant_diff_with_nan(df, feature_column, label_column):\n",
    "    feature_column_vs_label = df \\\n",
    "                 .withColumn(\"feature_column_unnormal\", F.when(F.col(feature_column).isNull(), 1).otherwise(0)) \\\n",
    "                 .agg(F.sum(F.when((F.col(\"feature_column_unnormal\") == 1) & (F.col(label_column) == 1),1).otherwise(0)).alias('feature_column_unnormal_label_column'),\n",
    "                      F.sum(F.when((F.col(\"feature_column_unnormal\") == 1) & (F.col(label_column) == 0),1).otherwise(0)).alias('feature_column_unnormal_not_label_column'),\n",
    "                      F.sum(F.when((F.col(\"feature_column_unnormal\") == 0) & (F.col(label_column) == 1),1).otherwise(0)).alias('feature_column_normal_label_column'),\n",
    "                      F.sum(F.when((F.col(\"feature_column_unnormal\") == 0) & (F.col(label_column) == 0),1).otherwise(0)).alias('feature_column_normal_not_label_column'))\n",
    "    feature_column_vs_label = feature_column_vs_label.select(F.round(F.col('feature_column_unnormal_label_column') / (F.col('feature_column_unnormal_label_column') + F.col('feature_column_unnormal_not_label_column')),2).alias(feature_column + '_unnormal_churn_rate'),\\\n",
    "                                                     F.round(F.col('feature_column_normal_label_column') / (F.col('feature_column_normal_label_column') + F.col('feature_column_normal_not_label_column')),2).alias(feature_column + '_normal_churn_rate'))\n",
    "    return feature_column_vs_label      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+---------------------------+\n",
      "|play_time_unnormal_churn_rate|play_time_normal_churn_rate|\n",
      "+-----------------------------+---------------------------+\n",
      "|                         0.28|                       0.27|\n",
      "+-----------------------------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show whether the churn rate of songs with 'nan' play_time and that of songs with “play_time >=0” are significant different\n",
    "whether_churn_rate_significant_diff_with_nan(df_play_song_length_cleaned, \"play_time\", \"label\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From the table above we find the churn rate of songs NULL play_time and that of songs with “play_time >=0” are not significant different, which means play_time = NULL appears randomly without pattern, so that we can replace 'nan' with mean value of “play_time >=0” records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### count negative play_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter play_time is not NULL and convert column 'play_time' to 'int'\n",
    "# df_play_play_time_not_nan = df_play.withColumn(\"play_time_is_nan\", F.when(F.col('play_time').isNull(), 1).otherwise(0))  \n",
    "df_play_play_time_not_nan = df_play_song_length_cleaned.filter(F.col('play_time').isNotNull())\\\n",
    "                            .withColumn('play_time',F.col('play_time').cast('int')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6 songs with negative play_time\n"
     ]
    }
   ],
   "source": [
    "# count negative play_time\n",
    "print(\"There are\", \n",
    "      df_play_play_time_not_nan.filter(F.col('play_time') < 0).count(),\\\n",
    "     \"songs with negative play_time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  As there are only 6 songs with negative play_time, we will manipulate them together with NULL play_time records, replace them with mean value of “play_time >=0” records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get the 99th of play_time as 'top_play_time_threshold', and calculate mean value of larger or equal to zero \"play_time\" records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order column 'play_time' and sort, then find 99% top value\n",
    "df_play_play_time_larger_or_equals_to_zero = df_play_play_time_not_nan.filter(F.col('play_time') >= 0)\n",
    "\n",
    "top_play_time_threshold_cut = int(df_play_play_time_larger_or_equals_to_zero.count() * 0.01)    # top 1%\n",
    "\n",
    "df_play_play_time_larger_or_equals_to_zero_cut = df_play_play_time_larger_or_equals_to_zero \\\n",
    "                                                        .sort(F.col(\"play_time\").desc()) \\\n",
    "                                                        .select(F.col('play_time')) \\\n",
    "                                                        .limit(top_play_time_threshold_cut)     # limit top 1% play_time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99th of play_time as top_play_time_threshold to remove outliers 11830\n"
     ]
    }
   ],
   "source": [
    "# get the 99th of play_time as top_play_time_threshold to remove outliers.\n",
    "top_play_time_threshold = df_play_play_time_larger_or_equals_to_zero_cut.sort(F.col(\"play_time\")).take(1)[0][0]\n",
    "print('99th of play_time as top_play_time_threshold to remove outliers',str(top_play_time_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean value of larger or equal to zero \"play_time\" records is 144.0\n"
     ]
    }
   ],
   "source": [
    "# filter play_time less than 'top_play_time_threshold' \n",
    "# then calculate mean value of play_time\n",
    "play_time_mean = df_play_play_time_larger_or_equals_to_zero\\\n",
    "                            .withColumn(\"play_time_is_outlier\", F.when(F.col('play_time') >= top_play_time_threshold, 1).otherwise(0)) \\\n",
    "                            .filter(F.col(\"play_time_is_outlier\") == 0) \\\n",
    "                            .agg(F.round(avg(col(\"play_time\")), 0).alias('play_time_mean'))\n",
    "\n",
    "play_time_mean = play_time_mean.take(1)[0][0]\n",
    "print('mean value of larger or equal to zero \"play_time\" records is', str(play_time_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace NULL and negative play_time with  'play_time_mean',  and remove outlier whose 'play_time' value less than 'top_play_time_threshold'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NULL play_time with 'play_time_mean'\n",
    "df_play_song_length_play_time_cleaned = df_play_song_length_cleaned.withColumn(\"play_time\", F.when(F.col('play_time').isNull(), play_time_mean).otherwise(F.col('play_time')))  \n",
    "df_play_song_length_play_time_cleaned = df_play_song_length_play_time_cleaned.withColumn(\"play_time\", F.when(F.col('play_time') == 'nan', play_time_mean).otherwise(F.col('play_time'))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_play_song_length_play_time_cleaned = df_play_song_length_play_time_cleaned.withColumn('play_time',F.col('play_time').cast('int')) \\\n",
    "                            .withColumn('play_time', F.when(F.col('play_time') < 0, play_time_mean).otherwise(F.col('play_time'))) \\\n",
    "                            .filter(F.col(\"play_time\") <= top_play_time_threshold) \n",
    "\n",
    "\n",
    "df_feature_window_cleaned = df_feature_window_cleaned.withColumn('play_time', F.col('play_time').cast('int'))\\\n",
    "                                  .filter(F.col('event') == 'P').filter(F.col('play_time') <= top_play_time_threshold) \\\n",
    "                                  .union(df_feature_window.filter(F.col('event') == 'D')) \\\n",
    "                                  .union(df_feature_window.filter(F.col('event') == 'S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------+-------+--------------------+------------------+-----------------+-------------------+\n",
      "|summary|                 uid|  event| device|             song_id|         play_time|      song_length|              label|\n",
      "+-------+--------------------+-------+-------+--------------------+------------------+-----------------+-------------------+\n",
      "|  count|             8182785|8182785|8182785|             8181042|           8182785|          8182785|            8182785|\n",
      "|   mean| 1.663321958387472E8|   null|   null|1.682889676009665...| 144.4299803795407|240.9282491230064|0.26507148849688705|\n",
      "| stddev|1.4871156627324002E7|   null|   null|4.679619339481816...|272.75805647582456|95.71229897828493| 0.4413712930063083|\n",
      "|    min|           100077577|      P|     ar|                  -1|               0.0|              1.0|                  0|\n",
      "|    max|            99850419|      P|     ip|             9999722|           11830.0|           1333.0|                  1|\n",
      "+-------+--------------------+-------+-------+--------------------+------------------+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_play_song_length_play_time_cleaned.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From the table above, we can see the statistics of 'play_time' and 'song_length' are more reasonable now. \n",
    "##### Next, we start to generate features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Generate Features A:  Play time percentage related features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Generate Features A1:  Proportion features of different level of play time percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate play time percentage proportion feature as:\n",
    "#### time_percentage_0_to_20,  time_percentage_20_to_40,  time_percentage_40_to_60,  time_percentage_60_to_80,  and time_percentage_larger_than_80 \n",
    "#### for example: time_percentage_0_to_20 means 0<=percentage<20, which is ratio of (number of played songs with play time percentage less than 20%) to (total number of played songs)  per uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate column 'play_time_percentage_of_song_length'\n",
    "df_play_song_length_play_time_cleaned = df_play_song_length_play_time_cleaned. \\\n",
    "                                            withColumn('play_time_percentage_of_song_length', \n",
    "                                                       F.round((F.col('play_time') / F.col('song_length')),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to generate percentage features for proportion of different level of play time percentage\n",
    "\n",
    "def percentage_proportion_feature_generation_by_percentage_level(df, play_time_percentage_list, percentage_gap):\n",
    "    \"\"\"\n",
    "    generate percentage features for proportion of different play time percentage level  \n",
    "    \"\"\"\n",
    "    df_feature = df \\\n",
    "        .filter(F.col('event') == 'P') \\\n",
    "        .groupBy('uid') \\\n",
    "        .agg(*[F.round(F.sum(F.when((F.col('Play_time_percentage_of_song_length') >= (play_time_percentage - percentage_gap) / 100) & (F.col('Play_time_percentage_of_song_length') < play_time_percentage / 100),1).otherwise(0))/ \\\n",
    "             F.count(F.col('Play_time_percentage_of_song_length')), 2) \\\n",
    "             .alias('time_percentage_' + str(play_time_percentage - percentage_gap) + '_to_' +str(play_time_percentage)) \\\n",
    "             for play_time_percentage in play_time_percentage_list]\n",
    "            ) # *[] opens list and make them comma separated\n",
    "    return df_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate frequency features for all event_list, time_window_list\n",
    "play_time_percentage_list = [20,40,60,80]\n",
    "percentage_gap = 20\n",
    "df_percentage_proportion_feature_list = []\n",
    "df_percentage_proportion_feature_list.append(percentage_proportion_feature_generation_by_percentage_level(df_play_song_length_play_time_cleaned, play_time_percentage_list, percentage_gap))\n",
    "df_percentage_proportion_feature = df_percentage_proportion_feature_list[0]\n",
    "df_percentage_proportion_feature = df_percentage_proportion_feature.withColumn(\"time_percentage_larger_than_80\",F.round(1 - F.col('time_percentage_0_to_20') - F.col('time_percentage_20_to_40')\\\n",
    "                                 - F.col('time_percentage_40_to_60')- F.col('time_percentage_60_to_80'), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------+------------------------+------------------------+------------------------+------------------------------+\n",
      "|      uid|time_percentage_0_to_20|time_percentage_20_to_40|time_percentage_40_to_60|time_percentage_60_to_80|time_percentage_larger_than_80|\n",
      "+---------+-----------------------+------------------------+------------------------+------------------------+------------------------------+\n",
      "|104777734|                    1.0|                     0.0|                     0.0|                     0.0|                           0.0|\n",
      "| 11596711|                   0.47|                    0.11|                    0.02|                    0.06|                          0.34|\n",
      "|118301183|                    0.7|                     0.1|                     0.0|                     0.0|                           0.2|\n",
      "|151294213|                   0.81|                     0.0|                     0.0|                    0.13|                          0.06|\n",
      "|166601616|                   0.29|                    0.12|                    0.08|                    0.09|                          0.42|\n",
      "+---------+-----------------------+------------------------+------------------------+------------------------+------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_percentage_proportion_feature.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list to concatenate features generated so far\n",
    "features_to_final_join = []\n",
    "features_to_final_join.append(df_percentage_proportion_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Generate Features A2: Acceleration features of different level of play time percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ratio of count of songs played with >=80 percentage of nearest 7 days to that of nearest 30 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate feature as ratio of count of songs played with >= 80 percentage of nearest 7 days to that of nearest 30 days\n",
    "shorter_time_window = 7\n",
    "longer_time_window = 30\n",
    "snapshot_date = feature_window_end_date\n",
    "df_percentage_acceleration_feature = df_play_song_length_play_time_cleaned \\\n",
    "                            .filter(F.col('play_time_percentage_of_song_length') >= 0.8) \\\n",
    "                            .groupBy('uid') \\\n",
    "                            .agg(F.round(F.sum(F.when((F.col('date') >= snapshot_date-datetime.timedelta(shorter_time_window-1)) & (F.col('date')<=snapshot_date),1).otherwise(0))/ \\\n",
    "                             F.sum(F.when((F.col('date') >= snapshot_date-datetime.timedelta(longer_time_window-1)) & (F.col('date')<=snapshot_date),1).otherwise(0)), 2) \\\n",
    "                            .alias('time_percentage_larger_than_80' + '_' + str(shorter_time_window) + 'd_over_' + str(longer_time_window) + 'd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------------------------------+\n",
      "|      uid|time_percentage_larger_than_80_7d_over_30d|\n",
      "+---------+------------------------------------------+\n",
      "| 11596711|                                      0.45|\n",
      "|118301183|                                       0.0|\n",
      "|151294213|                                       0.0|\n",
      "|166601616|                                      0.45|\n",
      "|167570658|                                       0.1|\n",
      "+---------+------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_percentage_acceleration_feature.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate features generated so far\n",
    "features_to_final_join.append(df_percentage_acceleration_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Generate Feature B: Play_time related feature "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Generate Features B1: Total play_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 'total_play_time'\n",
    "total_play_time_feature = df_play_song_length_play_time_cleaned \\\n",
    "        .groupBy('uid') \\\n",
    "        .agg(F.sum(F.col('play_time').cast('int')).alias('total_play_time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+\n",
      "|      uid|total_play_time|\n",
      "+---------+---------------+\n",
      "|104777734|             37|\n",
      "| 11596711|          11476|\n",
      "|118301183|            611|\n",
      "|151294213|            737|\n",
      "|166601616|           7238|\n",
      "+---------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_play_time_feature.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate features generated so far\n",
    "features_to_final_join.append(total_play_time_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Generate Features B2: Acceleration features of total play_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ratio of total play_time of nearest 7 days to that of nearest 30 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate feature as ratio of total number of play_time of nearest 7 days to that of nearest 30 days\n",
    "shorter_time_window = 7\n",
    "longer_time_window = 30\n",
    "df_total_play_time_acceleration_feature = df_play_song_length_play_time_cleaned \\\n",
    "                            .groupBy('uid') \\\n",
    "                            .agg(F.round(F.sum(F.when((F.col('date') >= snapshot_date-datetime.timedelta(shorter_time_window-1)) & (F.col('date')<=snapshot_date), F.col('play_time')).otherwise(0))/ \\\n",
    "                             F.sum(F.when((F.col('date') >= snapshot_date-datetime.timedelta(longer_time_window-1)) & (F.col('date')<=snapshot_date), F.col('play_time')).otherwise(0)),2) \\\n",
    "                            .alias('total_play_time' + '_' + str(shorter_time_window) + 'd_over_' + str(longer_time_window) + 'd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------------+\n",
      "|      uid|total_play_time_7d_over_30d|\n",
      "+---------+---------------------------+\n",
      "|104777734|                        0.0|\n",
      "| 11596711|                       0.47|\n",
      "|118301183|                        0.0|\n",
      "|151294213|                        0.0|\n",
      "|166601616|                       0.57|\n",
      "+---------+---------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_total_play_time_acceleration_feature.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate features generated so far\n",
    "features_to_final_join.append(df_total_play_time_acceleration_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Generate Features B3: Average play_time of songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_play_time_feature = df_play_song_length_play_time_cleaned \\\n",
    "        .groupBy('uid') \\\n",
    "        .agg(F.round(F.sum(F.col('play_time')) / \\\n",
    "                     F.count(F.col('play_time')), 2) \\\n",
    "             .alias('average_play_time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+\n",
      "|      uid|average_play_time|\n",
      "+---------+-----------------+\n",
      "|104777734|            12.33|\n",
      "| 11596711|            88.96|\n",
      "|118301183|             61.1|\n",
      "|151294213|            46.06|\n",
      "|166601616|            92.79|\n",
      "+---------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "average_play_time_feature.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate features generated so far\n",
    "features_to_final_join.append(average_play_time_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Generate Features C: Event related features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Generate Event features C1: events frequency in given windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to generate frequency features for a list of time windows\n",
    "# using when().otherwise(), and list comprehension trick!\n",
    "def frequency_feature_generation_time_windows(df,event,time_window_list,snapshot_date):\n",
    "    \"\"\"\n",
    "    generate frequency features for one event type and a list of time windows\n",
    "    \"\"\"\n",
    "    df_feature = df \\\n",
    "        .filter(F.col('event')==event) \\\n",
    "        .groupBy('uid') \\\n",
    "        .agg(*[F.sum(F.when((F.col('date')>=snapshot_date-datetime.timedelta(time_window-1)) & (F.col('date')<=snapshot_date),1).otherwise(0)).alias('freq_'+event+'_last_'+str(time_window)) \\\n",
    "                for time_window in time_window_list]\n",
    "            )# *[] opens list and make them comma separated\n",
    "    return df_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate frequency features for all event_list, time_window_list\n",
    "event_list = ['P','D','S']\n",
    "time_window_list = [1,3,7,14,30]\n",
    "df_event_frequrency_feature_list = []\n",
    "for event in event_list:\n",
    "    df_event_frequrency_feature_list.append(frequency_feature_generation_time_windows(df_feature_window_cleaned,event,time_window_list,snapshot_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DataFrame[uid: string, freq_P_last_1: bigint, freq_P_last_3: bigint, freq_P_last_7: bigint, freq_P_last_14: bigint, freq_P_last_30: bigint],\n",
       " DataFrame[uid: string, freq_D_last_1: bigint, freq_D_last_3: bigint, freq_D_last_7: bigint, freq_D_last_14: bigint, freq_D_last_30: bigint],\n",
       " DataFrame[uid: string, freq_S_last_1: bigint, freq_S_last_3: bigint, freq_S_last_7: bigint, freq_S_last_14: bigint, freq_S_last_30: bigint]]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_event_frequrency_feature_list    # list of three feature dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+-------------+-------------+--------------+--------------+\n",
      "|      uid|freq_P_last_1|freq_P_last_3|freq_P_last_7|freq_P_last_14|freq_P_last_30|\n",
      "+---------+-------------+-------------+-------------+--------------+--------------+\n",
      "| 81114900|            0|            0|            0|             0|            14|\n",
      "|168555344|            0|            0|           35|           111|           252|\n",
      "|168572740|            0|            0|            0|             0|             5|\n",
      "|168580671|           24|           97|          126|           276|          1524|\n",
      "|168610161|            0|            0|            0|             2|            49|\n",
      "+---------+-------------+-------------+-------------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_event_frequrency_feature_list[0].show(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 Generate Event features C2:  Ratio of event frequency of nearest 7 days to that of nearest 30 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to generate acceleration features for Ratio of event frequency of different time window (acceleration)\n",
    "def event_ratio_of_different_time_window(df,event, shorter_time_window, longer_time_window, snapshot_date):\n",
    "    \"\"\"\n",
    "    generate frequency features for Ratio of event frequency of different time window\n",
    "    \"\"\"\n",
    "    df_feature = df \\\n",
    "        .filter(F.col('event')==event) \\\n",
    "        .groupBy('uid') \\\n",
    "        .agg(F.round(F.sum(F.when((F.col('date') >= snapshot_date-datetime.timedelta(shorter_time_window-1)) & (F.col('date')<=snapshot_date),1).otherwise(0))/ \\\n",
    "             F.sum(F.when((F.col('date') >= snapshot_date-datetime.timedelta(longer_time_window-1)) & (F.col('date')<=snapshot_date),1).otherwise(0)), 2) \\\n",
    "             .alias(event + '_' + str(shorter_time_window) + 'd_over_' + event + '_'+ str(longer_time_window) + 'd')) \n",
    "    return df_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate recency features for all event_list\n",
    "event_list = ['P','D','S']\n",
    "shorter_time_window = 7\n",
    "longer_time_window = 30\n",
    "df_event_accerlaration_feature_list = []\n",
    "for event in event_list:\n",
    "    df_event_accerlaration_feature_list.append(event_ratio_of_different_time_window(df_feature_window_cleaned, event, shorter_time_window, longer_time_window, snapshot_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DataFrame[uid: string, P_7d_over_P_30d: double],\n",
       " DataFrame[uid: string, D_7d_over_D_30d: double],\n",
       " DataFrame[uid: string, S_7d_over_S_30d: double]]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_event_accerlaration_feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+\n",
      "|      uid|P_7d_over_P_30d|\n",
      "+---------+---------------+\n",
      "| 81114900|            0.0|\n",
      "|168555344|           0.14|\n",
      "|168572740|            0.0|\n",
      "|168580671|           0.08|\n",
      "|168610161|            0.0|\n",
      "+---------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_event_accerlaration_feature_list[0].show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Generate Features D: Recency related features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 Generate Recency features D1: Last Event Time from feature_window_end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to generate recency features for event time from a given Snapshot Time\n",
    "def last_event_time_from_snapshot_time(df,event,snapshot_date):\n",
    "    \"\"\"\n",
    "    generate frequency features for one event type and a list of time windows\n",
    "    \"\"\"\n",
    "    df_feature = df \\\n",
    "        .filter(F.col('event')==event) \\\n",
    "        .groupBy('uid') \\\n",
    "        .agg(datediff(lit(snapshot_date), F.max(F.col('date'))).alias('last_'+event+'_time_from_'+str(snapshot_date)))# *[] opens list and make them comma separated\n",
    "    return df_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate recency features for all event_list\n",
    "event_list = ['P','D','S']\n",
    "df_recency_feature_list = []\n",
    "for event in event_list:\n",
    "    df_recency_feature_list.append(last_event_time_from_snapshot_time(df_feature_window_cleaned,event,snapshot_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DataFrame[uid: string, last_P_time_from_2017-04-28: int],\n",
       " DataFrame[uid: string, last_D_time_from_2017-04-28: int],\n",
       " DataFrame[uid: string, last_S_time_from_2017-04-28: int]]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_recency_feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------------+\n",
      "|      uid|last_P_time_from_2017-04-28|\n",
      "+---------+---------------------------+\n",
      "| 81114900|                         29|\n",
      "|168555344|                          4|\n",
      "|168572740|                         29|\n",
      "|168580671|                          0|\n",
      "|168610161|                          9|\n",
      "+---------+---------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_recency_feature_list[0].show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Generate Features E: Profile related features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1 Generate Profile features E1: device_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_profile_tmp = df_feature_window_cleaned.select('uid', 'device')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check user uses multiple devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 20 users use multiple devices\n"
     ]
    }
   ],
   "source": [
    "# check user with multiple device value\n",
    "from pyspark.sql.functions import countDistinct\n",
    "uid_with_mutiple_devices = df_profile_tmp.groupBy('uid').agg(countDistinct(\"device\").alias('number_of_devices'))\n",
    "uid_with_mutiple_devices = uid_with_mutiple_devices.filter(F.col('number_of_devices') > 1)\n",
    "print(\"There are \" + str(uid_with_mutiple_devices.count()) + \" users use multiple devices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check user has no  'device' value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 users use has no devices value\n"
     ]
    }
   ],
   "source": [
    "# check user with no device value\n",
    "df_profile_tmp = df_profile_tmp.groupBy('uid') \\\n",
    "                    .agg(F.sum(F.when(F.col('device') == 'ar', 1).otherwise(0)).alias('number_of_ar'),\n",
    "                         F.sum(F.when(F.col('device') == 'ip', 1).otherwise(0)).alias('number_of_ip'))\n",
    "uid_with_no_devices = df_profile_tmp.filter(F.col('number_of_ar') == 0).filter(F.col('number_of_ip') == 0)\n",
    "print(\"There are \" + str(uid_with_no_devices.count()) + \" users use has no devices value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  As there are 20 users use multiple devices, we assigned the device label as device with more entries by correspond user, if user has the same entries number of 'ar' and 'ip', we assign its device label as 'ip'.\n",
    "##### Here we have device_feature with value '0' for 'ip' and '1' for 'ar'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_profile_tmp = df_profile_tmp.withColumn('device', F.when(F.col('number_of_ar') > F.col('number_of_ip'), 1).otherwise(0))\n",
    "df_device_feature = df_profile_tmp.select(F.col('uid'), F.col('device'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|      uid|device|\n",
      "+---------+------+\n",
      "| 81114900|     1|\n",
      "|168555344|     1|\n",
      "|168572740|     1|\n",
      "|168580671|     1|\n",
      "|168610161|     1|\n",
      "+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_device_feature.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate features generated so far\n",
    "features_to_final_join.append(df_device_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Form training data for prediction models"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "All faeutres as below:\n",
    "\n",
    "A1: df_percentage_proportion_feature\n",
    "A2: df_percentage_acceleration_feature\n",
    "\n",
    "B1: total_play_time_feature\n",
    "B2: df_total_play_time_acceleration_feature\n",
    "B3: average_play_time_feature\n",
    "    \n",
    "C1: df_event_frequrency_feature_list\n",
    "C2: df_event_accerlaration_feature_list\n",
    "    \n",
    "D1: df_recency_feature_list\n",
    "    \n",
    "E1: df_device_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to join list of tables\n",
    "def join_feature_data(df_master,df_feature_list):\n",
    "    for df_feature in df_feature_list:\n",
    "        df_master = df_master.join(df_feature,on='uid',how='left')\n",
    "        #df_master.persist() # uncomment if number of joins is too many\n",
    "    return df_master    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join all percentage related features, total play time related features and profile features.\n",
    "df_model_final = join_feature_data(df_label, features_to_final_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join all event related features and recency related features.\n",
    "df_model_final = join_feature_data(df_model_final, df_event_frequrency_feature_list)\n",
    "df_model_final = join_feature_data(df_model_final, df_event_accerlaration_feature_list)\n",
    "df_model_final = join_feature_data(df_model_final, df_recency_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_final.fillna(0).toPandas().to_csv('../data/df_model_final.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Form rating data for recommendation system "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We define 'rating' as the larger value of play_score and download_score：\n",
    "\n",
    "#### play_score:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"play_score\" are generate by 'play_time_percentage_of_song_length'.  \n",
    "The idea is that the larger percentage is, the more likely the user like the song, rules as below:   \n",
    "0.8 <= 'play_time_percentage_of_song_length', assign \"play_score\" 5  \n",
    "0.6 <= 'play_time_percentage_of_song_length' < 0.8, assign \"play_score\" 4  \n",
    "0.4 <= 'play_time_percentage_of_song_length' < 0.6, assign \"play_score\" 3  \n",
    "0.2 <= 'play_time_percentage_of_song_length'< 0.4, assign \"play_score\" 2  \n",
    "0 <= 'play_time_percentage_of_song_length' < 0.2, assign \"play_score\" 1  \n",
    "Note: If per uid per song_id has multiple ratings, we take average.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### download_score:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"download_score\" are generate by whether user has download entry in feature window: 2017-03-30 ~ 2017-04-28.  \n",
    "The idea is that if a user download a song, he has great probability to like the song, rules as below:   \n",
    "If have download entry, assign \"download_score\" 5  \n",
    "If no download entry, assign \"download_score\" 0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign 'df_rating'\n",
    "df_recommendation = df_play_song_length_play_time_cleaned \\\n",
    "                        .filter(F.col('song_id').isNotNull())  \\\n",
    "                        .withColumn('play_score', F.when(F.col('play_time_percentage_of_song_length') >= 0.8, 5)  \\\n",
    "                                    .otherwise(F.when(F.col('play_time_percentage_of_song_length') >= 0.6, 4)  \\\n",
    "                                    .otherwise(F.when(F.col('play_time_percentage_of_song_length') >= 0.4, 3)  \\\n",
    "                                    .otherwise(F.when(F.col('play_time_percentage_of_song_length') >= 0.2, 2)  \\\n",
    "                                    .otherwise(1))))) \n",
    "# generate rating table and take average.\n",
    "df_rating = df_recommendation.groupBy(F.col('uid'), F.col('song_id')) \\\n",
    "                            .agg(F.round(F.sum(F.col('play_score')) / F.count(F.col('play_score')), 0).alias('play_score'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign 'download_score'\n",
    "df_download_score = df_feature_window_cleaned \\\n",
    "                         .filter(F.col('event') == 'D')  \\\n",
    "                         .groupBy(F.col('uid'), F.col('song_id')).count() \n",
    "df_download_score = df_download_score.withColumn(\"download_score\", F.when(F.col(\"count\") >= 1, 5).otherwise(0))  \\\n",
    "                                     .select(F.col('uid'), F.col('song_id'), F.col('download_score'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join df_rating and df_download_score, and fill empty download_score as 0\n",
    "# generate 'rating' as larger value of play_score adn download_score\n",
    "# drop records with song_id is zero\n",
    "df_rating = df_rating.join(df_download_score, on=['uid', 'song_id'],how='left') \\\n",
    "                     .fillna(0)  \\\n",
    "                     .withColumn('rating', F.when(F.col('play_score') < F.col('download_score'), F.col('download_score')).otherwise(F.col('play_score'))) \\\n",
    "                     .select(F.col('uid'), F.col('song_id'), F.col('rating')) \\\n",
    "                     .filter(F.col('song_id') != 0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rating.toPandas().to_csv('../data/df_rating.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
